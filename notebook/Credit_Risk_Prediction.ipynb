{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q xgboost shap\n"
      ],
      "metadata": {
        "id": "4bI3UW8siZEs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S0nYlwVARuJP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/default_credit.csv\"\n",
        "TARGET_COL = \"Y\"     # correct label column\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/project_outputs\"\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 1. Load data\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Drop index-like column\n",
        "if \"Unnamed: 0\" in df.columns:\n",
        "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "print(\"Loaded data with shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# 2. Basic cleaning\n",
        "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Remove target from feature lists\n",
        "if TARGET_COL in num_cols:\n",
        "    num_cols.remove(TARGET_COL)\n",
        "if TARGET_COL in cat_cols:\n",
        "    cat_cols.remove(TARGET_COL)\n",
        "\n",
        "# Handle missing\n",
        "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
        "df[cat_cols] = df[cat_cols].fillna(\"MISSING\")\n",
        "# --- Safe cleaning for the target column Y ---\n",
        "# (paste this where you were getting the ValueError)\n",
        "\n",
        "# 1) Inspect a few distinct values in Y to see what's wrong\n",
        "print(\"Unique (sample) values in Y (first 20):\", pd.Series(df['Y'].unique()).head(20).tolist())\n",
        "\n",
        "# 2) Try coercing to numeric and see how many non-convertible values there are\n",
        "y_numeric = pd.to_numeric(df['Y'], errors='coerce')\n",
        "n_non_numeric = y_numeric.isna().sum()\n",
        "print(\"Non-convertible values in Y:\", n_non_numeric)\n",
        "\n",
        "if n_non_numeric > 0:\n",
        "    # Show examples of non-numeric entries (so you can inspect)\n",
        "    bad_values = df.loc[y_numeric.isna(), 'Y'].unique().tolist()\n",
        "    print(\"Examples of bad values in Y:\", bad_values)\n",
        "    # Common case: the very first row is an extra header like 'default payment next month'\n",
        "    # If so, drop that row(s). We'll drop any rows where Y cannot be converted to numeric.\n",
        "    df = df.loc[~y_numeric.isna()].copy()\n",
        "    # Recompute numeric column after dropping\n",
        "    df['Y'] = pd.to_numeric(df['Y'], errors='coerce')\n",
        "else:\n",
        "    # All good — assign numeric\n",
        "    df['Y'] = y_numeric\n",
        "\n",
        "# 3) Final cast to int (safe now because non-numeric rows were dropped)\n",
        "df['Y'] = df['Y'].astype(int)\n",
        "\n",
        "# 4) Quick sanity checks\n",
        "print(\"After cleaning, shape:\", df.shape)\n",
        "print(\"Target value counts:\\n\", df['Y'].value_counts())\n",
        "\n",
        "# If you dropped rows and want to know how many:\n",
        "# original_count = 30001  # or store before cleaning\n",
        "# print(\"Dropped rows:\", original_count - df.shape[0])\n",
        "\n",
        "# 3. Split\n",
        "X = df.drop(columns=[TARGET_COL])\n",
        "y = df[TARGET_COL].astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(\"Train/test sizes:\", X_train.shape, X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvaykO9SSAa-",
        "outputId": "7af4ab76-af75-4a30-c5e1-d7f9078b8042"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data with shape: (30001, 24)\n",
            "Columns: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'Y']\n",
            "Unique (sample) values in Y (first 20): ['default payment next month', '1', '0']\n",
            "Non-convertible values in Y: 1\n",
            "Examples of bad values in Y: ['default payment next month']\n",
            "After cleaning, shape: (30000, 24)\n",
            "Target value counts:\n",
            " Y\n",
            "0    23364\n",
            "1     6636\n",
            "Name: count, dtype: int64\n",
            "Train/test sizes: (24000, 23) (6000, 23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_map = {\n",
        "    \"X1\": \"LIMIT_BAL\",\n",
        "    \"X2\": \"SEX\",\n",
        "    \"X3\": \"EDUCATION\",\n",
        "    \"X4\": \"MARRIAGE\",\n",
        "    \"X5\": \"AGE\",\n",
        "    \"X6\": \"PAY_0\",\n",
        "    \"X7\": \"PAY_2\",\n",
        "    \"X8\": \"PAY_3\",\n",
        "    \"X9\": \"PAY_4\",\n",
        "    \"X10\": \"PAY_5\",\n",
        "    \"X11\": \"PAY_6\",\n",
        "    \"X12\": \"BILL_AMT1\",\n",
        "    \"X13\": \"BILL_AMT2\",\n",
        "    \"X14\": \"BILL_AMT3\",\n",
        "    \"X15\": \"BILL_AMT4\",\n",
        "    \"X16\": \"BILL_AMT5\",\n",
        "    \"X17\": \"BILL_AMT6\",\n",
        "    \"X18\": \"PAY_AMT1\",\n",
        "    \"X19\": \"PAY_AMT2\",\n",
        "    \"X20\": \"PAY_AMT3\",\n",
        "    \"X21\": \"PAY_AMT4\",\n",
        "    \"X22\": \"PAY_AMT5\",\n",
        "    \"X23\": \"PAY_AMT6\"\n",
        "}\n",
        "\n",
        "df = df.rename(columns=human_map)\n",
        "X = df.drop(columns=[TARGET_COL])\n",
        "y = df[TARGET_COL]\n",
        "\n",
        "feature_cols = X.columns.tolist()\n",
        "print(\"Features:\", feature_cols)\n",
        "\n",
        "# Update train/test splits because column names changed\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lmr9oHwfi7zl",
        "outputId": "4671482a-60e8-4eac-92f7-a52279ce16d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: ['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numeric_features = feature_cols\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[(\"num\", numeric_transformer, numeric_features)]\n",
        ")\n",
        "\n",
        "# Compute class imbalance ratio\n",
        "num_pos = y_train.sum()\n",
        "num_neg = len(y_train) - num_pos\n",
        "scale_pos_weight = num_neg / num_pos\n",
        "print(\"scale_pos_weight =\", scale_pos_weight)\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=RANDOM_STATE,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"clf\", xgb_clf)\n",
        "])\n",
        "\n",
        "print(\"Training model…\")\n",
        "pipeline.fit(X_train, y_train)\n",
        "print(\"Model trained.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOy_JDuSjCxE",
        "outputId": "932aa894-f492-4b53-e263-393bec34053f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scale_pos_weight = 3.520625353173856\n",
            "Training model…\n",
            "Model trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# Predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)   # <-- REQUIRED for Cell 9\n",
        "\n",
        "# Print results\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "print(\"\\nClassification report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSXlVZDxjLBP",
        "outputId": "022f4cf7-61af-47fc-d2dc-0d0af8a1f9dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.7761\n",
            "Precision: 0.4725\n",
            "Recall: 0.6142\n",
            "F1-score: 0.5341\n",
            "Confusion matrix:\n",
            " [[3763  910]\n",
            " [ 512  815]]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.81      0.84      4673\n",
            "           1       0.47      0.61      0.53      1327\n",
            "\n",
            "    accuracy                           0.76      6000\n",
            "   macro avg       0.68      0.71      0.69      6000\n",
            "weighted avg       0.79      0.76      0.77      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_test_trans = pipeline.named_steps[\"preproc\"].transform(X_test)\n",
        "model = pipeline.named_steps[\"clf\"]\n",
        "\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test_trans)\n",
        "\n",
        "# SHAP summary plot\n",
        "plt.figure(figsize=(10,6))\n",
        "shap.summary_plot(shap_values, X_test_trans, feature_names=numeric_features, show=False)\n",
        "plt.savefig(f\"{OUTPUT_DIR}/shap_summary.png\", dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# SHAP bar plot\n",
        "plt.figure(figsize=(10,6))\n",
        "shap.summary_plot(shap_values, X_test_trans, feature_names=numeric_features, plot_type=\"bar\", show=False)\n",
        "plt.savefig(f\"{OUTPUT_DIR}/shap_bar.png\", dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"SHAP global plots saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN0xugeMjQRb",
        "outputId": "062cd507-2796-4a9f-c286-4972e5dcfbf1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHAP global plots saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analysis = X_test.copy()\n",
        "analysis[\"y_true\"] = y_test.values\n",
        "analysis[\"y_pred\"] = y_pred\n",
        "analysis[\"y_proba\"] = y_proba\n",
        "\n",
        "low_risk_idx = analysis[analysis[\"y_true\"]==0].sort_values(\"y_proba\").index[0]\n",
        "high_risk_idx = analysis[analysis[\"y_true\"]==1].sort_values(\"y_proba\", ascending=False).index[0]\n",
        "\n",
        "fn = analysis[(analysis[\"y_true\"]==1) & (analysis[\"y_pred\"]==0)]\n",
        "fp = analysis[(analysis[\"y_true\"]==0) & (analysis[\"y_pred\"]==1)]\n",
        "\n",
        "if len(fn)>0:\n",
        "    surprising_idx = fn.index[0]\n",
        "elif len(fp)>0:\n",
        "    surprising_idx = fp.index[0]\n",
        "else:\n",
        "    surprising_idx = analysis.index[len(analysis)//2]\n",
        "\n",
        "selected = {\n",
        "    \"low_risk\": low_risk_idx,\n",
        "    \"high_risk\": high_risk_idx,\n",
        "    \"surprising\": surprising_idx\n",
        "}\n",
        "\n",
        "selected\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82lIR1vIjP-o",
        "outputId": "9c5e6d40-c21d-4791-b776-84202de0fb69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'low_risk': np.int64(1384),\n",
              " 'high_risk': np.int64(16555),\n",
              " 'surprising': np.int64(2157)}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL corrected cell: one safe loop to generate per-example SHAP outputs & explanations\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "local_results = {}\n",
        "\n",
        "# quick sanity prints\n",
        "print(\"shap_values shape:\", getattr(shap_values, \"shape\", None))\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"selected:\", selected)\n",
        "\n",
        "for name, orig_idx in selected.items():\n",
        "    # map original label index -> positional index for X_test / shap_values\n",
        "    try:\n",
        "        pos = X_test.index.get_loc(orig_idx)\n",
        "    except KeyError:\n",
        "        # if orig_idx is already a positional index (or X_test was reset), use it directly\n",
        "        try:\n",
        "            pos = int(orig_idx)\n",
        "            print(f\"Note: original index {orig_idx} not found; using positional {pos} as fallback.\")\n",
        "        except Exception as e:\n",
        "            raise KeyError(f\"Can't interpret selected index {orig_idx}: {e}\")\n",
        "\n",
        "    # safety check\n",
        "    if pos < 0 or pos >= shap_values.shape[0]:\n",
        "        raise IndexError(f\"Positional index {pos} out of bounds for shap_values length {shap_values.shape[0]}\")\n",
        "\n",
        "    # fetch shap row and original features (positional)\n",
        "    sv = shap_values[pos]\n",
        "    orig = X_test.iloc[pos]\n",
        "\n",
        "    # build local DataFrame (sorted by absolute SHAP)\n",
        "    local_df = pd.DataFrame({\n",
        "        \"feature\": numeric_features,\n",
        "        \"value\": orig.values,\n",
        "        \"shap_value\": sv\n",
        "    })\n",
        "    local_df[\"abs_shap\"] = local_df[\"shap_value\"].abs()\n",
        "    local_df = local_df.sort_values(\"abs_shap\", ascending=False).drop(columns=[\"abs_shap\"])\n",
        "\n",
        "    # save CSV\n",
        "    csv_path = os.path.join(OUTPUT_DIR, f\"local_shap_{name}.csv\")\n",
        "    local_df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # save horizontal bar plot of top 10 contributors (signed)\n",
        "    topk = local_df.head(10).sort_values(\"shap_value\")\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.barh(topk[\"feature\"], topk[\"shap_value\"])\n",
        "    plt.xlabel(\"SHAP value\")\n",
        "    plt.title(f\"Top SHAP contributors ({name})\")\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(OUTPUT_DIR, f\"local_shap_bar_{name}.png\")\n",
        "    plt.savefig(plot_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # prepare textual explanation using the 'analysis' DataFrame when possible\n",
        "    if orig_idx in analysis.index:\n",
        "        pred_proba = float(analysis.loc[orig_idx, \"y_proba\"])\n",
        "        true_label = int(analysis.loc[orig_idx, \"y_true\"])\n",
        "    else:\n",
        "        pred_proba = float(analysis.iloc[pos][\"y_proba\"])\n",
        "        true_label = int(analysis.iloc[pos][\"y_true\"])\n",
        "\n",
        "    top_pos = local_df[local_df[\"shap_value\"] > 0].head(5)\n",
        "    top_neg = local_df[local_df[\"shap_value\"] < 0].head(5)\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"--- {name.upper()} CASE (orig index {orig_idx}, pos {pos}) ---\")\n",
        "    lines.append(f\"True label: {true_label}, Predicted probability: {pred_proba:.4f}\\n\")\n",
        "    lines.append(\"Top 5 risk-increasing features:\")\n",
        "    for _, r in top_pos.iterrows():\n",
        "        lines.append(f\"  + {r['feature']} = {r['value']}, SHAP={r['shap_value']:.4f}\")\n",
        "    lines.append(\"\\nTop 5 risk-decreasing features:\")\n",
        "    for _, r in top_neg.iterrows():\n",
        "        lines.append(f\"  - {r['feature']} = {r['value']}, SHAP={r['shap_value']:.4f}\")\n",
        "\n",
        "    explanation_text = \"\\n\".join(lines)\n",
        "    txt_path = os.path.join(OUTPUT_DIR, f\"explanation_{name}.txt\")\n",
        "    with open(txt_path, \"w\") as f:\n",
        "        f.write(explanation_text)\n",
        "\n",
        "    # store and print\n",
        "    local_results[name] = {\"pos\": pos, \"csv\": csv_path, \"plot\": plot_path, \"txt\": txt_path, \"df\": local_df}\n",
        "    print(f\"\\n{name.upper()} EXPLANATION (orig index {orig_idx}, pos {pos}):\")\n",
        "    print(local_df.head(10))\n",
        "\n",
        "print(\"\\nAll per-example outputs saved to:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2is7ih14n0Ro",
        "outputId": "cec82896-a3c9-4a16-fa3a-eeee88f95787"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shap_values shape: (6000, 23)\n",
            "X_test shape: (6000, 23)\n",
            "selected: {'low_risk': np.int64(1384), 'high_risk': np.int64(16555), 'surprising': np.int64(2157)}\n",
            "\n",
            "LOW_RISK EXPLANATION (orig index 1384, pos 5368):\n",
            "      feature   value  shap_value\n",
            "18   PAY_AMT2  120927   -0.945953\n",
            "17   PAY_AMT1   83754   -0.736917\n",
            "5       PAY_0      -1   -0.424108\n",
            "0   LIMIT_BAL  300000   -0.357485\n",
            "19   PAY_AMT3       0   -0.248942\n",
            "11  BILL_AMT1   42189   -0.165250\n",
            "15  BILL_AMT5   82475   -0.163355\n",
            "4         AGE      27   -0.162768\n",
            "22   PAY_AMT6   92250   -0.155413\n",
            "21   PAY_AMT5   25213   -0.090043\n",
            "\n",
            "HIGH_RISK EXPLANATION (orig index 16555, pos 469):\n",
            "      feature  value  shap_value\n",
            "5       PAY_0      3    1.385520\n",
            "10      PAY_6      7    0.423067\n",
            "0   LIMIT_BAL  10000    0.329796\n",
            "14  BILL_AMT4   2300    0.249434\n",
            "6       PAY_2      2    0.224244\n",
            "11  BILL_AMT1   2300    0.193294\n",
            "19   PAY_AMT3      0    0.184054\n",
            "9       PAY_5      7    0.183551\n",
            "8       PAY_4      7    0.162810\n",
            "7       PAY_3      2    0.162629\n",
            "\n",
            "SURPRISING EXPLANATION (orig index 2157, pos 3):\n",
            "      feature   value  shap_value\n",
            "5       PAY_0       0   -0.565761\n",
            "22   PAY_AMT6   23333   -0.397136\n",
            "20   PAY_AMT4   15000    0.298099\n",
            "12  BILL_AMT2  303701    0.269844\n",
            "18   PAY_AMT2   10500   -0.269208\n",
            "11  BILL_AMT1  305823    0.261739\n",
            "19   PAY_AMT3   10000   -0.206327\n",
            "16  BILL_AMT6  230925    0.203585\n",
            "13  BILL_AMT3  296384    0.171721\n",
            "14  BILL_AMT4  248801    0.126264\n",
            "\n",
            "All per-example outputs saved to: /content/drive/MyDrive/project_outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick cell to (re)compute feat_imp and save top5 + executive summary\n",
        "import os\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ensure OUTPUT_DIR exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- compute feat_imp from shap_values if missing ---\n",
        "# shap_values: ndarray shape (n_test, n_features)\n",
        "# numeric_features: list of feature names (length n_features)\n",
        "try:\n",
        "    feat_imp  # if exists, do nothing\n",
        "    print(\"feat_imp already defined.\")\n",
        "except NameError:\n",
        "    print(\"Computing feat_imp from shap_values...\")\n",
        "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
        "    feat_imp = pd.DataFrame({\n",
        "        \"feature\": numeric_features,\n",
        "        \"mean_abs_shap\": mean_abs_shap\n",
        "    }).sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
        "    print(\"Computed feat_imp.\")\n",
        "\n",
        "# Save top-5 global SHAP\n",
        "top5 = feat_imp.head(5)\n",
        "top5_path = os.path.join(OUTPUT_DIR, \"top5_shap.csv\")\n",
        "top5.to_csv(top5_path, index=False)\n",
        "print(\"Saved top-5 SHAP ->\", top5_path)\n",
        "print(\"Top-5 features:\\n\", top5)\n",
        "\n",
        "# If metrics (auc, precision, recall, f1, cm, report) are missing, warn\n",
        "missing = [v for v in [\"auc\",\"precision\",\"recall\",\"f1\",\"cm\",\"report\",\"X_train\",\"X_test\"] if v not in globals()]\n",
        "if missing:\n",
        "    print(\"Warning: the following variables are not in session and are required for the executive summary:\", missing)\n",
        "    print(\"If these are missing, re-run the model evaluation cell (Cell 5) before proceeding.\")\n",
        "else:\n",
        "    # Executive summary (auto-generate)\n",
        "    exec_path = os.path.join(OUTPUT_DIR, \"executive_summary.txt\")\n",
        "    exec_text = f\"\"\"Executive summary — Default of Credit Card Clients (auto-generated)\n",
        "Generated: {datetime.now().isoformat()}\n",
        "\n",
        "Model: XGBoost classifier\n",
        "Train / Test sizes: {X_train.shape[0]} / {X_test.shape[0]}\n",
        "\n",
        "Key metrics (test):\n",
        "- AUC: {auc:.4f}\n",
        "- Precision: {precision:.4f}\n",
        "- Recall: {recall:.4f}\n",
        "- F1: {f1:.4f}\n",
        "\n",
        "Top 5 global SHAP drivers:\n",
        "{chr(10).join([f'{i+1}. {row.feature} (mean|SHAP|={row.mean_abs_shap:.6f})' for i,row in top5.reset_index(drop=True).iterrows()])}\n",
        "\n",
        "Business recommendations:\n",
        "1) Use the top SHAP drivers in underwriting rules and manual review workflows.\n",
        "2) For surprising cases flagged by the model, require additional manual checks.\n",
        "3) Monitor feature drift and SHAP distribution monthly; retrain if performance drops.\n",
        "4) Consider threshold adjustment for operational balance between recall/precision.\n",
        "\n",
        "Artifacts saved in: {OUTPUT_DIR}\n",
        "\"\"\"\n",
        "    with open(exec_path, \"w\") as f:\n",
        "        f.write(exec_text)\n",
        "    print(\"Saved executive summary ->\", exec_path)\n",
        "\n",
        "    # Print short summary for quick copy/paste\n",
        "    print(\"\\n=== Quick summary ===\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1: {f1:.4f}\")\n",
        "    print(\"\\n--- Top 5 SHAP ---\")\n",
        "    print(top5.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5vtE9r7Htjc",
        "outputId": "f19c3526-bf50-45f7-b731-d9f86e1c7f54"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing feat_imp from shap_values...\n",
            "Computed feat_imp.\n",
            "Saved top-5 SHAP -> /content/drive/MyDrive/project_outputs/top5_shap.csv\n",
            "Top-5 features:\n",
            "      feature  mean_abs_shap\n",
            "0      PAY_0       0.554533\n",
            "1  LIMIT_BAL       0.224891\n",
            "2  BILL_AMT1       0.158520\n",
            "3   PAY_AMT2       0.127405\n",
            "4   PAY_AMT1       0.114219\n",
            "Saved executive summary -> /content/drive/MyDrive/project_outputs/executive_summary.txt\n",
            "\n",
            "=== Quick summary ===\n",
            "AUC: 0.7761\n",
            "Precision: 0.4725\n",
            "Recall: 0.6142\n",
            "F1: 0.5341\n",
            "\n",
            "--- Top 5 SHAP ---\n",
            "  feature  mean_abs_shap\n",
            "    PAY_0       0.554533\n",
            "LIMIT_BAL       0.224891\n",
            "BILL_AMT1       0.158520\n",
            " PAY_AMT2       0.127405\n",
            " PAY_AMT1       0.114219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9 — Save final metrics, top-5 SHAP, and executive summary\n",
        "import os\n",
        "from datetime import datetime\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Metrics (assumes auc, precision, recall, f1, cm, report variables exist)\n",
        "metrics_path = os.path.join(OUTPUT_DIR, \"metrics.txt\")\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    f.write(\"Model evaluation metrics (test set)\\n\")\n",
        "    f.write(\"===============================\\n\")\n",
        "    f.write(f\"AUC: {auc:.4f}\\n\")\n",
        "    f.write(f\"Precision: {precision:.4f}\\n\")\n",
        "    f.write(f\"Recall: {recall:.4f}\\n\")\n",
        "    f.write(f\"F1: {f1:.4f}\\n\\n\")\n",
        "    f.write(\"Confusion matrix:\\n\")\n",
        "    f.write(np.array2string(cm))\n",
        "    f.write(\"\\n\\nClassification report:\\n\")\n",
        "    f.write(report)\n",
        "print(\"Saved metrics ->\", metrics_path)\n",
        "\n",
        "# Top-5 global SHAP (feat_imp DataFrame assumed to exist)\n",
        "top5 = feat_imp.head(5)\n",
        "top5_path = os.path.join(OUTPUT_DIR, \"top5_shap.csv\")\n",
        "top5.to_csv(top5_path, index=False)\n",
        "print(\"Saved top-5 SHAP ->\", top5_path)\n",
        "print(\"Top-5 features:\\n\", top5)\n",
        "\n",
        "# Executive summary (auto-generate, customize as needed)\n",
        "exec_path = os.path.join(OUTPUT_DIR, \"executive_summary.txt\")\n",
        "exec_text = f\"\"\"Executive summary — Default of Credit Card Clients (auto-generated)\n",
        "Generated: {datetime.now().isoformat()}\n",
        "\n",
        "Model: XGBoost classifier\n",
        "Train / Test sizes: {X_train.shape[0]} / {X_test.shape[0]}\n",
        "\n",
        "Key metrics (test):\n",
        "- AUC: {auc:.4f}\n",
        "- Precision: {precision:.4f}\n",
        "- Recall: {recall:.4f}\n",
        "- F1: {f1:.4f}\n",
        "\n",
        "Top 5 global SHAP drivers:\n",
        "{chr(10).join([f'{i+1}. {row.feature} (mean|SHAP|={row.mean_abs_shap:.6f})' for i,row in top5.reset_index(drop=True).iterrows()])}\n",
        "\n",
        "Business recommendations:\n",
        "1) Use the top SHAP drivers in underwriting rules and manual review workflows.\n",
        "2) For surprising cases flagged by the model, require additional manual checks.\n",
        "3) Monitor feature drift and SHAP distribution monthly; retrain if performance drops.\n",
        "4) Consider threshold adjustment for operational balance between recall/precision.\n",
        "\n",
        "Artifacts saved in: {OUTPUT_DIR}\n",
        "\"\"\"\n",
        "with open(exec_path, \"w\") as f:\n",
        "    f.write(exec_text)\n",
        "print(\"Saved executive summary ->\", exec_path)\n",
        "\n",
        "# Print short summary for quick copy/paste\n",
        "print(\"\\n=== Quick summary ===\")\n",
        "print(open(metrics_path).read())\n",
        "print(\"--- Top 5 SHAP ---\")\n",
        "print(top5.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2mUgU4MrN7X",
        "outputId": "85570e69-3da1-4814-bf42-cc56d769ecc9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved metrics -> /content/drive/MyDrive/project_outputs/metrics.txt\n",
            "Saved top-5 SHAP -> /content/drive/MyDrive/project_outputs/top5_shap.csv\n",
            "Top-5 features:\n",
            "      feature  mean_abs_shap\n",
            "0      PAY_0       0.554533\n",
            "1  LIMIT_BAL       0.224891\n",
            "2  BILL_AMT1       0.158520\n",
            "3   PAY_AMT2       0.127405\n",
            "4   PAY_AMT1       0.114219\n",
            "Saved executive summary -> /content/drive/MyDrive/project_outputs/executive_summary.txt\n",
            "\n",
            "=== Quick summary ===\n",
            "Model evaluation metrics (test set)\n",
            "===============================\n",
            "AUC: 0.7761\n",
            "Precision: 0.4725\n",
            "Recall: 0.6142\n",
            "F1: 0.5341\n",
            "\n",
            "Confusion matrix:\n",
            "[[3763  910]\n",
            " [ 512  815]]\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.81      0.84      4673\n",
            "           1       0.47      0.61      0.53      1327\n",
            "\n",
            "    accuracy                           0.76      6000\n",
            "   macro avg       0.68      0.71      0.69      6000\n",
            "weighted avg       0.79      0.76      0.77      6000\n",
            "\n",
            "--- Top 5 SHAP ---\n",
            "  feature  mean_abs_shap\n",
            "    PAY_0       0.554533\n",
            "LIMIT_BAL       0.224891\n",
            "BILL_AMT1       0.158520\n",
            " PAY_AMT2       0.127405\n",
            " PAY_AMT1       0.114219\n"
          ]
        }
      ]
    }
  ]
}